pub_date	title	venue	excerpt	citation	url_slug	paper_url	slides_url
2023-05-17	Mixup Training for Generative Models to Defend Membership Inference Attack	IEEE International Conference on Computer Communications (INFOCOM)	"With the popularity of machine learning, it has been a growing concern on the trained model revealing the private information of the training data. Membership inference attack (MIA) poses one of the threats by inferring whether a given sample participates in the training of the target model. Although MIA has been widely studied for discriminative models, for generative models, neither it nor its defense is extensively investigated. In this work, we propose a mixup training method for generative adversarial networks (GANs) as a defense against MIAs. Specifically, the original training data is replaced with their interpolations so that GANs would never overfit the original data. The intriguing part is an analysis from the hypothesis test perspective to theoretically prove our method could mitigate the AUC of the strongest likelihood ratio attack. Experimental results support that mixup training successfully defends the state-of-the-art MIAs for generative models, yet without model performance degradation or any additional training efforts, showing great promise to be deployed in practice."	"Zhe Ji; Qiansiqi Hu; Liyao Xiang*; Chenghu Zhou; ""Mixup Training for Generative Models to Defend Membership Inference Attacks"", in Proc. IEEE International Conference on Computer Communications (INFOCOM), New York area, USA, May 17-20, 2023."	mixup	https://doi.org/10.1109/INFOCOM53939.2023.10229036	/files/slides_mixup.pdf
2024-02-26	Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models	Network and Distributed System Security (NDSS) Symposium	"With the increased capabilities at the edge (e.g., mo- bile device) and more stringent privacy requirement, it becomes a recent trend for deep learning-enabled applications to pre-process sensitive raw data at the edge and transmit the features to the backend cloud for further processing. A typical application is to run machine learning (ML) services on facial images collected from different individuals. To prevent identity theft, conventional methods commonly rely on an adversarial game-based approach to shed the identity information from the feature. However, such methods can not defend against adaptive attacks, in which an attacker takes a countermove against a known defence strategy. We propose Crafter, a feature crafting mechanism deployed at the edge, to protect the identity information from adaptive model inversion attacks while ensuring the ML tasks are properly carried out in the cloud. The key defence strategy is to mislead the attacker to a non-private prior from which the attacker gains little about the private identity. In this case, the crafted features act like poison training samples for attackers with adaptive model updates. Experimental results indicate that Crafter successfully defends both basic and possible adaptive attacks, which can not be achieved by state-of-the-art adversarial game-based methods"	"Shiming Wang; Zhe Ji; Liyao Xiang*; Hao Zhang; Xinbing Wang; Chenghu Zhou; Bo Li; ""Crafter: Facial Feature Crafting against Inversion-based Identity Theft on Deep Models"", in Network and Distributed System Security (NDSS) Symposium, San Diego, USA, Feb.26-Mar.1, 2024. "	crafter	https://dx.doi.org/10.14722/ndss.2024.23326	
2024-10-28	A Principled Approach to Natural Language Watermarking	ACM International Conference on Multimedia (MM)	"Recently, there has been a surge in machine-generated natural language content being misused by unauthorized parties. Watermarking is a well-recognized technique to address the issue by tracing the provenance of the text. However, we found that most existing watermarking systems for texts are subject to ad hoc design and thus suffer from fundamental vulnerabilities. We propose a principled design for text watermarking based on a theoretical information-hiding framework. The watermarking party and attacker play a rate-distortion-constrained capacity game to achieve the maximum rate of reliable transmission, i.e., watermark capacity. The capacity can be expressed by the mutual information between the encoding and the attacker's corrupted text, indicating how many watermark bits are effectively conveyed under distortion constraints. The system is realized by a learning-based framework with mutual information neural estimators. In the framework, we adopt the assumption of an omniscient attacker and let the watermarking party pit against the attacker who is fully aware of the watermarking strategy. The watermarking party thus achieves higher robustness against removal attacks. We further show that the incorporation of side information substantially enhances the efficacy and robustness of the watermarking system. Experimental results have shown the superiority of our watermarking system compared to the state-of-the-art in terms of capacity, robustness, and preserving text semantics."	"Zhe Ji; Qiansiqi Hu; Yicheng Zheng; Liyao Xiang*; Xinbing Wang; ""A Principled Approach to Natural Language Watermarking"", In Proceedings of the 32nd ACM International Conference on Multimedia (MM '24), Melbourne, VIC, Australia, Oct.28-Nov.1, 2024."	watermark	https://doi.org/10.1145/3664647.3681544	
